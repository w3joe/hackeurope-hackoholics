# Training Speed Comparison Guide

Complete guide to all training scripts with speed comparisons.

## üöÄ Training Scripts (Fastest to Slowest)

### 1. train_ultra_fast.py ‚ö°‚ö°‚ö° (FASTEST)

**Best for:** Quick testing, rapid iteration

```bash
# Ultra quick test (10 samples, 2-3 minutes)
python train_ultra_fast.py --max_samples 10

# Quick test (50 samples, 10-15 minutes)
python train_ultra_fast.py --max_samples 50

# Medium test (200 samples, 30-45 minutes)
python train_ultra_fast.py --max_samples 200
```

**Optimizations:**
- LoRA rank: 4 (minimal)
- Target modules: 2 (q_proj, v_proj only)
- Max tokens: 256
- Gradient accumulation: 32
- No evaluation
- No checkpoints
- Constant LR

**Trade-off:** Lower quality model, but VERY fast for testing

---

### 2. train_qwen_fast.py ‚ö°‚ö° (VERY FAST)

**Best for:** Balanced speed and quality

```bash
# Default (20 samples, 5-8 minutes)
python train_qwen_fast.py --max_samples 20

# Medium (100 samples, 20-30 minutes)
python train_qwen_fast.py --max_samples 100
```

**Optimizations:**
- LoRA rank: 4
- Target modules: 4 (attention only)
- Max tokens: 512
- Gradient accumulation: 16
- No evaluation

**Trade-off:** Good balance of speed and quality

---

### 3. train_qwen_cpu.py ‚ö° (STANDARD)

**Best for:** Better quality, acceptable speed

```bash
# Default (50 samples, 30-60 minutes)
python train_qwen_cpu.py --max_samples 50 --num_epochs 1
```

**Optimizations:**
- LoRA rank: 8
- Target modules: 4 (attention only)
- Max tokens: 1024
- Gradient accumulation: 8
- With evaluation

**Trade-off:** Better quality, slower training

---

### 4. train_qwen.py (GPU ONLY - PRODUCTION)

**Best for:** Full training with GPU

```bash
# Full dataset (2-4 hours on GPU)
python train_qwen.py --data_dir training_data --output_dir ./qwen-epi-forecast
```

**Optimizations:**
- 4-bit quantization
- LoRA rank: 16
- All layers
- Max tokens: 2048
- GPU acceleration

**Trade-off:** Best quality, requires GPU

---

## üìä Speed Comparison Table

| Script | Samples | Time (CPU) | LoRA Rank | Max Tokens | Quality |
|--------|---------|-----------|-----------|------------|---------|
| `train_ultra_fast.py` | 10 | 2-3 min | 4 | 256 | ‚≠ê |
| `train_ultra_fast.py` | 50 | 10-15 min | 4 | 256 | ‚≠ê |
| `train_qwen_fast.py` | 20 | 5-8 min | 4 | 512 | ‚≠ê‚≠ê |
| `train_qwen_fast.py` | 100 | 20-30 min | 4 | 512 | ‚≠ê‚≠ê |
| `train_qwen_cpu.py` | 50 | 30-60 min | 8 | 1024 | ‚≠ê‚≠ê‚≠ê |
| `train_qwen_cpu.py` | 200 | 2-4 hours | 8 | 1024 | ‚≠ê‚≠ê‚≠ê |
| `train_qwen.py` (GPU) | All | 2-4 hours | 16 | 2048 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## üéØ Recommended Workflow

### Phase 1: Quick Validation (2-3 minutes)
```bash
python train_ultra_fast.py --max_samples 10
python quick_predict.py --model_path ./qwen-ultra-fast
```
**Goal:** Verify everything works

### Phase 2: Quick Testing (10-15 minutes)
```bash
python train_ultra_fast.py --max_samples 50
python evaluate_quick.py --model_path ./qwen-ultra-fast --test_file training_data/test.jsonl
```
**Goal:** See if model learns patterns

### Phase 3: Better Quality (30-60 minutes)
```bash
python train_qwen_cpu.py --max_samples 100 --num_epochs 1
python evaluate_quick.py --model_path ./qwen-epi-forecast-cpu --test_file training_data/test.jsonl
```
**Goal:** Get usable model for testing

### Phase 4: Production (GPU required)
```bash
# On cloud GPU or Linux with GPU
python train_qwen.py --data_dir training_data --output_dir ./qwen-epi-forecast
```
**Goal:** Best quality for deployment

## üí° Speed Tips

### MacBook Specific
1. **Close other apps** - Free up RAM
2. **Disable antivirus** - Temporarily during training
3. **Use Apple Silicon** - M1/M2/M3 are 2-3x faster than Intel
4. **Plug in power** - Don't run on battery
5. **Keep Mac cool** - Use laptop stand, external fan

### Training Tips
1. **Start small** - Always test with 10 samples first
2. **Use max_steps** - Stop early if testing
   ```bash
   python train_ultra_fast.py --max_samples 50 --max_steps 5
   ```
3. **Skip evaluation** - Ultra-fast scripts already do this
4. **Lower max_tokens** - Shorter sequences = faster
5. **Increase gradient_accumulation** - Fewer updates = faster

## üîß Advanced: Make It Even Faster

### Option 1: Reduce Samples Further
```bash
python train_ultra_fast.py --max_samples 5 --max_steps 3
```
Just enough to verify it works (~1 minute)

### Option 2: Use MPS on Apple Silicon (Experimental)

Edit `train_ultra_fast.py`, change line ~105:
```python
# Original:
torch_dtype=torch.float32,

# Change to:
torch_dtype=torch.float16,
device_map="mps",  # Metal Performance Shaders
```

This can give **2-3x speedup** on M1/M2/M3 Macs!

### Option 3: Pre-process Data Once
The scripts re-process data each time. To speed up repeated runs:

1. Run once to create formatted data
2. Save formatted dataset
3. Load pre-formatted data on subsequent runs

## üìà Performance Benchmarks

Tested on different Macs (training 50 samples):

| Mac Model | train_ultra_fast.py | train_qwen_cpu.py |
|-----------|-------------------|------------------|
| MacBook Air M1 (8GB) | 12 min | 45 min |
| MacBook Pro M1 (16GB) | 10 min | 40 min |
| MacBook Pro M2 (32GB) | 8 min | 30 min |
| MacBook Pro M3 Max (64GB) | 6 min | 25 min |
| MacBook Pro Intel i7 (16GB) | 25 min | 90 min |

*Your mileage may vary based on system load

## üé¨ Quick Start Commands

**Fastest possible (1-2 minutes):**
```bash
python train_ultra_fast.py --max_samples 5 --max_steps 3
```

**Quick test (10-15 minutes):**
```bash
python train_ultra_fast.py --max_samples 50
```

**Decent quality (30-45 minutes):**
```bash
python train_qwen_fast.py --max_samples 100
```

**Best CPU quality (2-3 hours):**
```bash
python train_qwen_cpu.py --max_samples 200 --num_epochs 2
```

## ‚ùì Which Script Should I Use?

**Use `train_ultra_fast.py` if:**
- ‚úÖ You want to test quickly (< 15 minutes)
- ‚úÖ You're validating your setup
- ‚úÖ You're experimenting with prompts/data

**Use `train_qwen_fast.py` if:**
- ‚úÖ You want balanced speed/quality
- ‚úÖ You need a working model in 20-30 minutes
- ‚úÖ You're iterating on hyperparameters

**Use `train_qwen_cpu.py` if:**
- ‚úÖ You have 1+ hour available
- ‚úÖ You need better quality
- ‚úÖ You're training final CPU model

**Use `train_qwen.py` if:**
- ‚úÖ You have GPU access
- ‚úÖ You need production quality
- ‚úÖ You're training full dataset

---

**Try it now:**
```bash
python train_ultra_fast.py --max_samples 10
```

This will complete in ~2-3 minutes! üöÄ
