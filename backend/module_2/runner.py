"""Module 2 â€” Pharmacy Inventory Gap Analyzer."""

import json
import time
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

from jinja2 import Environment, FileSystemLoader
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI

BACKEND_ROOT = Path(__file__).resolve().parent.parent
PROMPTS_DIR = BACKEND_ROOT / "prompts"


def _get_llm():
    from src.config import (
        API_KEY,
        GEMINI_MODEL,
        MODULE_2_REQUEST_TIMEOUT,
        MODULE_2_TEMPERATURE,
    )

    if not API_KEY:
        raise ValueError("GOOGLE_API_KEY or GEMINI_API_KEY must be set")
    return ChatGoogleGenerativeAI(
        model=GEMINI_MODEL,
        temperature=MODULE_2_TEMPERATURE,
        api_key=API_KEY,
        timeout=MODULE_2_REQUEST_TIMEOUT,
    )


def _batches(items: list, size: int):
    """Yield successive chunks of items of length size."""
    for i in range(0, len(items), size):
        yield items[i : i + size]


def _load_inventory() -> dict:
    from module_2.loader import load_vaccine_inventory
    return load_vaccine_inventory()


def _validate_cost_arithmetic(report: dict) -> list[str]:
    """Validate estimated_restock_cost_usd = recommended_quantity * unit_cost per gap."""
    errors = []
    total_reported = report.get("total_estimated_restock_cost_usd", 0)
    total_computed = 0.0
    for gap in report.get("critical_gaps", []):
        rec_qty = gap.get("recommended_quantity", 0)
        # We need supplier_unit_cost - it may be in the gap or we use a placeholder
        # The gap has preferred_supplier_id but not unit cost. The LLM should output estimated_restock_cost_usd.
        # Validation: estimated_restock_cost_usd should equal recommended_quantity * unit_cost.
        # We don't have unit_cost in the gap schema - the LLM outputs estimated_restock_cost_usd.
        # So we can't independently verify without looking up supplier data. PRD says "validate post-generation".
        # We'll check: sum(estimated_restock_cost_usd) == total_estimated_restock_cost_usd
        total_computed += gap.get("estimated_restock_cost_usd", 0)
    if abs(total_computed - total_reported) > 0.01:  # float tolerance
        errors.append(
            f"total_estimated_restock_cost_usd ({total_reported}) != sum of gap costs ({total_computed})"
        )
    return errors


def _run_batch(
    batch_inv: dict,
    risk_context: list[dict],
    system_prompt: str,
    user_tpl,
    llm,
    module_name: str,
) -> list[dict]:
    """Run gap analysis for a single batch of pharmacies. Returns gap_reports for that batch. Retries on parse/invoke errors."""
    from src.utils.logging import Timer, log_llm_call

    user_prompt = user_tpl.render(
        inventory_json=json.dumps(batch_inv, indent=2),
        risk_context_json=json.dumps(risk_context, indent=2),
    )

    from src.config import MODULE_2_REQUEST_TIMEOUT

    last_error = None
    for attempt in range(3):  # Up to 3 attempts (initial + 2 retries)
        retry = attempt > 0

        try:
            with Timer() as timer:
                with ThreadPoolExecutor(max_workers=1) as ex:
                    future = ex.submit(
                        llm.invoke,
                        [
                            SystemMessage(content=system_prompt),
                            HumanMessage(content=user_prompt),
                        ],
                    )
                    response = future.result(timeout=MODULE_2_REQUEST_TIMEOUT)

            validation_errors = []
            for report in response.gap_reports:
                validation_errors.extend(_validate_cost_arithmetic(report.model_dump()))

            if validation_errors and not retry:
                user_prompt += f"\n\n[RETRY] Cost validation errors: {'; '.join(validation_errors)}. Ensure total_estimated_restock_cost_usd equals the sum of estimated_restock_cost_usd across all critical_gaps."
                continue

            output = response.model_dump()
            for report in output["gap_reports"]:
                total = sum(
                    g.get("estimated_restock_cost_usd", 0) for g in report.get("critical_gaps", [])
                )
                report["total_estimated_restock_cost_usd"] = round(total, 2)

            log_llm_call(
                module=module_name,
                latency_ms=timer.elapsed_ms,
                retry_triggered=retry,
                validation_errors=validation_errors if validation_errors else None,
            )
            return output["gap_reports"]

        except Exception as e:
            last_error = e
            err_str = str(e).lower()
            retriable = (
                attempt < 2
                and (
                    "parse" in err_str
                    or "completion" in err_str
                    or "output" in err_str
                    or "timeout" in err_str
                    or "timed out" in err_str
                )
            )
            if retriable:
                time.sleep(5)  # Back off before retry
                continue
            raise

    raise last_error


def run_module_2(
    risk_assessments: list[dict],
    inventory: dict | None = None,
    batch_size: int | None = None,
) -> dict:
    """
    Run Module 2: Pharmacy Inventory Gap Analyzer.

    risk_assessments: from Module 1B (country, risk_level, recommended_disease_focus)
    inventory: optional override; otherwise loaded from vaccine_stock_dataset.csv
    batch_size: pharmacies per LLM call. Default from MODULE_2_BATCH_SIZE env (20).

    Output: { "gap_reports": [...] } or error object
    """
    from src.config import MODULE_2_BATCH_SIZE
    from src.schemas.module_2 import Module2Output

    module_name = "module_2"
    input_snapshot = {"risk_assessments": risk_assessments}
    effective_batch_size = batch_size if batch_size is not None else MODULE_2_BATCH_SIZE
    effective_batch_size = max(1, effective_batch_size)

    try:
        inv = inventory or _load_inventory()
        pharmacies = inv.get("pharmacies", [])
        if not pharmacies:
            return {"gap_reports": []}

        risk_context = [
            {
                "country": r.get("country"),
                "risk_level": r.get("risk_level"),
                "recommended_disease_focus": r.get("recommended_disease_focus", []),
            }
            for r in risk_assessments
            if "country" in r
        ]

        env = Environment(loader=FileSystemLoader(str(PROMPTS_DIR)))
        system_tpl = env.get_template("module_2_system.jinja2")
        user_tpl = env.get_template("module_2_user.jinja2")
        system_prompt = system_tpl.render()

        llm = _get_llm()
        structured_llm = llm.with_structured_output(Module2Output, method="json_schema")

        from src.config import MODULE_2_BATCH_DELAY

        all_gap_reports: list[dict] = []
        batch_list = list(_batches(pharmacies, effective_batch_size))

        for batch_idx, batch_pharmacies in enumerate(batch_list):
            if batch_idx > 0 and MODULE_2_BATCH_DELAY > 0:
                time.sleep(MODULE_2_BATCH_DELAY)
            if len(batch_list) > 1:
                print(f"Module 2: batch {batch_idx + 1}/{len(batch_list)}...", flush=True)
            reports = _run_batch(
                batch_inv={"pharmacies": batch_pharmacies},
                risk_context=risk_context,
                system_prompt=system_prompt,
                user_tpl=user_tpl,
                llm=structured_llm,
                module_name=module_name,
            )
            all_gap_reports.extend(reports)

        return {"gap_reports": all_gap_reports}

    except Exception as e:
        return {
            "module": module_name,
            "error": str(e),
            "input_snapshot": input_snapshot,
        }
